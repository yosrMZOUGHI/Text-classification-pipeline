{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usual Data science pipeline for text classification step by step "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step zero: Problem definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are dealing with a supervised learning classification problem with a binary output ( target classes): either True or False. The data we are manipulating is extracted from a csv file and is in text format. Therefore some Natural Language Processing techniques will be applied when preparing our dataset before modeling. \n",
    "\n",
    "NB: We have an imbalanced data: the classes are not equally distributed ( approximatively 70-30%), which is a frequent problem in classification. To evaluate the impact of this imbalance on the output of our model, we can use the confusion matrix to see if the dominant class is predicted more often when it the actual class is the least dominant one. \n",
    "However, we will not fix this problem. Refer to this link for some useful techniques: https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re  \n",
    "import nltk \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords  \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/zeroeffort/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/zeroeffort/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step one: Data collection  (Importing The dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(input_path):\n",
    "    # assertion to verify the input file existance\n",
    "    assert os.path.isfile(input_path) != False , \"No valid input file: file does not exist\" \n",
    "    try: \n",
    "        df= pd.read_csv(input_path,encoding = \"ISO-8859-1\")\n",
    "    \n",
    "    except exception as e:\n",
    "        pass\n",
    "    finally:\n",
    "        return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step two: Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with raw text data is not a winning shot! First, we need to clean our data then vectorize it (transform it to numerical matrix so it can be passed to our ML algorithm). \n",
    "\n",
    "1-Many techniques for text data cleaning exist. Most often, we will need to apply an NLP pipeline: Sentence segmentation, Tokenization, Lemmatization, Removing special characters, Case conversion, etc. The following class will introduce some of the most used techniques with libraries such as nltk , re and Spacy which proposes an interesting packaged pipeline.\n",
    "\n",
    "2-Different approaches exist to convert text into the corresponding numerical form. The Bag of Words Model and the Word Embedding Model are two of the most commonly used approaches. For time shortage, we will only use tf-idf of sklearn. Also, the Fast Text library and Spacy provide very performant word vectors to represent text data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class text_prep():\n",
    "    \n",
    "    def __init__(self, cleaning= 'nltk', input_path= None):\n",
    "        assert cleaning in ['nltk', 'spacy'], \" text_cleaning value must be either nltk or Spacy.\"\n",
    "        assert input_path != None, \"No input path given\"\n",
    "    \n",
    "        text_cleaning = { 'nltk': self.text_cleaning_nltk, 'spacy': self.text_cleaning_spacy}\n",
    "       \n",
    "            \n",
    "        self.df= import_data(input_path)\n",
    "        self.documents= text_cleaning[cleaning]()\n",
    "        self.X= self.convert_text()\n",
    "        self.Y= self.df[\"Classes\"]\n",
    "        \n",
    "        \n",
    "\n",
    "    def text_cleaning_nltk(self):\n",
    "        documents = []\n",
    "        try:\n",
    "            \n",
    "            \n",
    "            stemmer = WordNetLemmatizer()\n",
    "\n",
    "            for sen in self.df.Content.values:  \n",
    "                # Remove all the special characters\n",
    "                data = re.sub(r'\\W', ' ', str(sen))\n",
    "\n",
    "                # remove all single characters\n",
    "                data = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', data)\n",
    "\n",
    "                # Remove single characters from the start\n",
    "                data = re.sub(r'\\^[a-zA-Z]\\s+', ' ', data) \n",
    "\n",
    "                # Substituting multiple spaces with single space\n",
    "                data = re.sub(r'\\s+', ' ', data, flags=re.I)\n",
    "\n",
    "                # Removing prefixed 'b'\n",
    "                data = re.sub(r'^b\\s+', '', data)\n",
    "\n",
    "                # Converting to Lowercase\n",
    "                data = data.lower()\n",
    "\n",
    "                # Lemmatization\n",
    "                data = data.split()\n",
    "\n",
    "                data = [stemmer.lemmatize(word) for word in data]\n",
    "                data = ' '.join(data)\n",
    "\n",
    "                documents.append(data)\n",
    "        except exception as e:\n",
    "                pass\n",
    "        finally:\n",
    "                return(documents)\n",
    "        \n",
    "        \n",
    "    def text_cleaning_spacy(self):\n",
    "        import spacy #load spacy\n",
    "        nlp = spacy.load(\"en\", disable=['parser', 'tagger', 'ner'])\n",
    "        stops = stopwords.words(\"english\")\n",
    "        documents=[]\n",
    "        content= self.df[\"Content\"]\n",
    "        for data in content.values:\n",
    "            data = data.lower()\n",
    "            data = nlp(data)\n",
    "            lemmatized = list()\n",
    "            for word in data:\n",
    "                lemma = word.lemma_.strip()\n",
    "                if lemma:\n",
    "                    lemmatized.append(lemma)\n",
    "       \n",
    "            documents.append(\" \".join(lemmatized))\n",
    "            \n",
    "        return (documents)\n",
    "        \n",
    "    # Converting Text to Numbers\n",
    "    def convert_text(self):\n",
    "        tfidfconverter = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))  \n",
    "        X = tfidfconverter.fit_transform(self.documents).toarray() \n",
    "        return(X)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Training Text Classification Model and Predicting Classes \n",
    "&\n",
    "# Step 4: Model Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these techniques to build and estimate the skill of machine learning models:\n",
    "1- With train split (80-20%) then evaluating the accuracy rate and the confusion matrix to visualize the performance of the chosen algorithm. \n",
    "2- Using cross-validation techniques (that generally have a lower bias than other methods) such as k-fold. \n",
    "To evaluate the results, we use confusion matrix and classification report (accuracy , precision and recall rates) provided by Sklearn.\n",
    "\n",
    "In the following class, we will implement one method to train and evaluate one model with the 80-20 technique and we will implement a second one to compare several algorithms using k-fold. \n",
    "\n",
    "Of course, each model has several parameters. we can use Grid Search technique to find the best parameters that can give us the best results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class build_eval_model():\n",
    "    def __init__(self, actual_X= None, actual_Y= None, model_name=\"\"):\n",
    "        # prepare models\n",
    "        self.classifier_dict= { \n",
    "                            \"RF\": RandomForestClassifier(),\n",
    "                            \"LR\": LogisticRegression(),\n",
    "                            \"CART\": DecisionTreeClassifier(),\n",
    "                            \"SVM\": SVC()\n",
    "                             }\n",
    "        \n",
    "        assert len(actual_X) !=0 and len(actual_Y != 0), \"Empty values passed to model.\"\n",
    "        assert model_name in self.classifier_dict.keys() or model_name == \"compare\", \"Import for this algorithm was not provided or no algorithm name was given\" \n",
    "        self.actual_X , self.actual_Y= actual_X, actual_Y\n",
    "        if (model_name == \"compare\"):\n",
    "            self.compare_models()\n",
    "        else:\n",
    "            self.build_one_model(model_name)\n",
    "        \n",
    "        \n",
    "    def build_one_model(self, model_name=\"\"):\n",
    "        \n",
    "        # Splitting Training and Test Sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.actual_X, self.actual_Y, test_size=0.2, random_state=0) \n",
    "        \n",
    "        classifier = self.classifier_dict[model_name]  \n",
    "        classifier.fit(X_train, y_train)  \n",
    "        y_pred = classifier.predict(X_test)\n",
    "        \n",
    "        # Evaluating the Model\n",
    "        print(confusion_matrix(y_test,y_pred))  \n",
    "        print(classification_report(y_test,y_pred))  \n",
    "        print(accuracy_score(y_test, y_pred))\n",
    "        \n",
    "    def compare_models (self):\n",
    "        \n",
    "        # evaluate each model in turn\n",
    "        results = []\n",
    "        names = []\n",
    "        scoring = 'accuracy'\n",
    "        for name, model in self.classifier_dict.items():\n",
    "            kfold = model_selection.KFold(n_splits=10, random_state=5)\n",
    "            # evaluating and saving cross_val results\n",
    "            cv_results = model_selection.cross_val_score(model, self.actual_X , self.actual_Y, cv=kfold, scoring=scoring)\n",
    "            results.append(cv_results)\n",
    "            names.append(name)\n",
    "            msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "            print(msg)\n",
    "            \n",
    "        # boxplot algorithm comparison and save it in /data/output folder\n",
    "        fig = plt.figure()\n",
    "        fig.suptitle('Algorithm Comparison')\n",
    "        ax = fig.add_subplot(111)\n",
    "        plt.boxplot(results)\n",
    "        ax.set_xticklabels(names)\n",
    "        fig.savefig('data/output/comparision.png')   # save the figure to file\n",
    "        plt.close(fig)    # close the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 : Linking it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run (input_path=\"\", model_name=\"\", cleaning=\"\"):\n",
    "   \n",
    "        text = text_prep(cleaning= cleaning, input_path= input_path)\n",
    "        build_eval_model(actual_X= text.X, \n",
    "                         actual_Y= text.Y, \n",
    "                         model_name= model_name)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1127    2]\n",
      " [  20   50]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.98      1.00      0.99      1129\n",
      "       True       0.96      0.71      0.82        70\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1199\n",
      "\n",
      "0.981651376146789\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    input_path= \"./data/input/exerciceDS.csv\"\n",
    "    model_name= \"LR\"\n",
    "    cleaning = \"spacy\"\n",
    "    \n",
    "    run (input_path= input_path,\n",
    "         model_name= model_name ,\n",
    "         cleaning= cleaning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
